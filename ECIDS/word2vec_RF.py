import numpy as np
import pandas as pd
import os
from gensim.models import Word2Vec
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm
import time
import warnings
import joblib
warnings.filterwarnings("ignore")

# ========== è®¾ç½®è·¯å¾„ ==========
train_path = "./data/UNSW/train/Payload_data_UNSW_train_split_binary.csv"
test_path = "./data/UNSW/test/Payload_data_UNSW_test_split_binary.csv"
w2v_model_path = "model/Joint/UNSW.model"
rf_model_path = "model/Joint/UNSW.pkl"

# ========== åŠ è½½æ•°æ® ==========
if not os.path.exists(train_path) or not os.path.exists(test_path):
    raise FileNotFoundError("âŒ è®­ç»ƒæˆ–æµ‹è¯•é›†æ–‡ä»¶æœªæ‰¾åˆ°")

df_train_full = pd.read_csv(train_path)
df_test_full = pd.read_csv(test_path)

# ========== åˆ†å±‚æŠ½æ ·å‡½æ•° ==========
def stratified_sample(df, label_col, sample_size):
    grouped = df.groupby(label_col, group_keys=False)
    total_size = len(df)
    frac = sample_size / total_size
    return grouped.apply(lambda x: x.sample(
        frac=frac,
        replace=(frac > 1),
        random_state=42
    ))

# ========== é‡‡æ · ==========
df_train = stratified_sample(df_train_full, 'label', 100_000)
df_test = stratified_sample(df_test_full, 'label', 10_000)

print(f"âœ… è®­ç»ƒé›†é‡‡æ ·å®Œæˆ: {len(df_train)} æ¡")
print(df_train['label'].value_counts(normalize=True))
print(f"âœ… æµ‹è¯•é›†é‡‡æ ·å®Œæˆ: {len(df_test)} æ¡")
print(df_test['label'].value_counts(normalize=True))

# ========== æå– payload å­—æ®µ ==========
feature_cols = [col for col in df_train.columns if col.startswith("payload_byte_")]
if len(feature_cols) != 1500:
    raise ValueError("âŒ payload å­—æ®µæ•°ä¸ä¸º1500ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
print(f"âœ… payload æå–å®Œæˆ")

# ========== è½¬æ¢ä¸º token åºåˆ— ==========
tokens_train = df_train[feature_cols].astype(str).values.tolist()
tokens_test = df_test[feature_cols].astype(str).values.tolist()
print(f"âœ… token åºåˆ—å®Œæˆ")

# ========== è®­ç»ƒ Word2Vec ==========
w2v = Word2Vec(sentences=tokens_train, vector_size=32, window=3, min_count=1)
os.makedirs(os.path.dirname(w2v_model_path), exist_ok=True)
w2v.save(w2v_model_path)
print(f"âœ… Word2Vec æ¨¡å‹å·²ä¿å­˜ä¸º: {w2v_model_path}")

# ========== åµŒå…¥å‡½æ•° ==========
def embed(tokens):
    vecs = [w2v.wv[t] for t in tokens if t in w2v.wv]
    return np.mean(vecs, axis=0) if vecs else np.zeros(w2v.vector_size)

train_embeds = np.vstack([embed(seq) for seq in tokens_train])
test_embeds = np.vstack([embed(seq) for seq in tokens_test])
print("âœ… åµŒå…¥å®Œæˆï¼Œè®­ç»ƒé›†:", train_embeds.shape, "æµ‹è¯•é›†:", test_embeds.shape)

# ========== æ¨¡å‹è®­ç»ƒ ==========
y_train = df_train['label'].values
y_test = df_test['label'].values

rf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)
print("ğŸŒ² æ­£åœ¨è®­ç»ƒéšæœºæ£®æ—...")
start = time.time()
rf.fit(train_embeds, y_train)
print("âœ… è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ {:.2f} ç§’".format(time.time() - start))

# ========== ä¿å­˜ RF æ¨¡å‹ ==========
joblib.dump(rf, rf_model_path)
print(f"ğŸ’¾ RF æ¨¡å‹å·²ä¿å­˜ä¸º: {rf_model_path}")

# ========== æ¨¡å‹é¢„æµ‹ ==========
print("ğŸ” æ­£åœ¨è¿›è¡Œé¢„æµ‹...")
y_pred = rf.predict(test_embeds)

acc = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred, digits=4)
print("ğŸ¯ æµ‹è¯•é›†å‡†ç¡®ç‡: {:.4f}".format(acc))
print("ğŸ“‹ åˆ†ç±»æŠ¥å‘Š:\n", report)
