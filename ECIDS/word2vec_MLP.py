import numpy as np
import pandas as pd
import os
from gensim.models import Word2Vec
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import time, warnings, joblib
warnings.filterwarnings("ignore")

# ========== è®¾ç½®è·¯å¾„ ==========
train_path = "./data/UNSW/train/Payload_data_UNSW_train_split_binary.csv"
test_path  = "./data/UNSW/test/Payload_data_UNSW_test_split_binary.csv"
w2v_model_path = "model/Joint/UNSW.model"
mlp_model_path = "model/Joint/UNSW_MLP.pkl"

# ========== åŠ è½½æ•°æ® ==========
if not os.path.exists(train_path) or not os.path.exists(test_path):
    raise FileNotFoundError("âŒ è®­ç»ƒæˆ–æµ‹è¯•é›†æ–‡ä»¶æœªæ‰¾åˆ°")

df_train_full = pd.read_csv(train_path)
df_test_full  = pd.read_csv(test_path)

# ========== åˆ†å±‚æŠ½æ ·å‡½æ•° ==========
def stratified_sample(df, label_col, sample_size):
    grouped = df.groupby(label_col, group_keys=False)
    frac = sample_size / len(df)
    return grouped.apply(lambda x: x.sample(frac=frac,
                                            replace=(frac > 1),
                                            random_state=42))

df_train = stratified_sample(df_train_full, 'label', 100_000)
df_test  = stratified_sample(df_test_full,  'label', 10_000)

# ========== æå– payload å­—æ®µ ==========
feature_cols = [c for c in df_train.columns if c.startswith("payload_byte_")]
if len(feature_cols) != 1500:
    raise ValueError("âŒ payload å­—æ®µæ•°ä¸ä¸º1500ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")

# ========== è½¬æ¢ä¸º token åºåˆ— ==========
tokens_train = df_train[feature_cols].astype(str).values.tolist()
tokens_test  = df_test[feature_cols].astype(str).values.tolist()

# ========== è®­ç»ƒ Word2Vec ==========
w2v = Word2Vec(sentences=tokens_train,
               vector_size=32, window=3, min_count=1)
os.makedirs(os.path.dirname(w2v_model_path), exist_ok=True)
w2v.save(w2v_model_path)
print(f"âœ… Word2Vec å·²ä¿å­˜: {w2v_model_path}")

# ========== åµŒå…¥å‡½æ•° ==========
def embed(tokens):
    vecs = [w2v.wv[t] for t in tokens if t in w2v.wv]
    return np.mean(vecs, axis=0) if vecs else np.zeros(w2v.vector_size)

X_train = np.vstack([embed(seq) for seq in tokens_train])
X_test  = np.vstack([embed(seq) for seq in tokens_test])
y_train = df_train['label'].values
y_test  = df_test['label'].values

# â˜…â˜…â˜…ï¼ˆå¯é€‰ï¼‰ç‰¹å¾æ ‡å‡†åŒ– â˜…â˜…â˜…
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)

# ========== è®­ç»ƒ MLP ==========
mlp = MLPClassifier(hidden_layer_sizes=(128, 64),
                    activation='relu',
                    solver='adam',
                    batch_size='auto',
                    learning_rate_init=1e-3,
                    max_iter=50,
                    random_state=42,
                    early_stopping=True,
                    verbose=False)
print("ğŸ¤– æ­£åœ¨è®­ç»ƒ MLP...")
start = time.time()
mlp.fit(X_train, y_train)
print(f"âœ… è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ {time.time() - start:.2f} ç§’")

# ========== ä¿å­˜æ¨¡å‹ ==========
joblib.dump({'model': mlp, 'scaler': scaler}, mlp_model_path)
print(f"ğŸ’¾ MLP æ¨¡å‹å·²ä¿å­˜ä¸º: {mlp_model_path}")

# ========== è¯„ä¼° ==========
print("ğŸ” å¼€å§‹é¢„æµ‹...")
y_pred = mlp.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"ğŸ¯ æµ‹è¯•é›†å‡†ç¡®ç‡: {acc:.4f}")
print("ğŸ“‹ åˆ†ç±»æŠ¥å‘Š:\n", classification_report(y_test, y_pred, digits=4))
